{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "env = Monitor(gym.make(\"CartPole-v1\"))\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=5e-4,\n",
    "    buffer_size=200000,\n",
    "    learning_starts=5000,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    target_update_interval=1000,\n",
    "    exploration_fraction=0.4,\n",
    "    exploration_final_eps=0.05,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=400000)\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20)\n",
    "print(\"Baseline eval:\", mean_reward, \"+/-\", std_reward)\n",
    "model.save(\"dqn_cartpole_baseline\")\n",
    "env.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "\n",
    "class ObsNoise(gym.ObservationWrapper):\n",
    "    def __init__(self, env, sigma=0.02):\n",
    "        super().__init__(env)\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return obs + np.random.normal(0, self.sigma, size=obs.shape)\n",
    "\n",
    "class ActionDelay(gym.Wrapper):\n",
    "    def __init__(self, env, delay_steps=2):\n",
    "        super().__init__(env)\n",
    "        self.delay_steps = delay_steps\n",
    "        self.queue = deque([0]*delay_steps, maxlen=delay_steps) \n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.queue = deque([0]*self.delay_steps, maxlen=self.delay_steps)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.queue.append(action)\n",
    "        delayed_action = self.queue[0]\n",
    "        return self.env.step(delayed_action)\n",
    "def make_noisy_delayed_env(sigma=0.02, delay_steps=2):\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    env = ObsNoise(env, sigma=sigma)\n",
    "    env = ActionDelay(env, delay_steps=delay_steps)\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import gymnasium as gym\n",
    "\n",
    "clean_env = gym.make(\"CartPole-v1\")\n",
    "hard_env = make_noisy_delayed_env(sigma=0.03, delay_steps=2)\n",
    "\n",
    "baseline = DQN.load(\"dqn_cartpole_baseline\", env=clean_env)\n",
    "\n",
    "print(\"Baseline on clean:\", evaluate_policy(baseline, clean_env, n_eval_episodes=20))\n",
    "print(\"Baseline on hard :\", evaluate_policy(baseline, hard_env, n_eval_episodes=20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "\n",
    "class DomainRandomize(gym.Wrapper):\n",
    "    def __init__(self, env_fn, sigma_range=(0.0, 0.05), delay_range=(0, 2)):\n",
    "        # env_fn should make a *fresh* base env\n",
    "        self.env_fn = env_fn\n",
    "        self.sigma_range = sigma_range\n",
    "        self.delay_range = delay_range\n",
    "        super().__init__(env_fn())\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        # swap env each episode\n",
    "        self.env.close()\n",
    "        sigma = random.uniform(*self.sigma_range)\n",
    "        delay = random.randint(*self.delay_range)\n",
    "        base = self.env_fn()\n",
    "        wrapped = ObsNoise(base, sigma=sigma)\n",
    "        wrapped = ActionDelay(wrapped, delay_steps=delay)\n",
    "        self.env = wrapped\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "def env_fn():\n",
    "    return gym.make(\"CartPole-v1\")\n",
    "\n",
    "train_env = DomainRandomize(env_fn, sigma_range=(0.0, 0.05), delay_range=(0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "robust = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    learning_rate=1e-3,\n",
    "    buffer_size=200_000,\n",
    "    learning_starts=5_000,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    target_update_interval=1_000,\n",
    "    exploration_fraction=0.3,\n",
    "    exploration_final_eps=0.05,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "robust.learn(total_timesteps=300_000)\n",
    "robust.save(\"dqn_cartpole_robust\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
